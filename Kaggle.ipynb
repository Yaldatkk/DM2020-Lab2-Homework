{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nData Preprocessing:\\nReplace some special text by regularize words.\\nCNN-LSTM model:\\nFirst I didn't do any preprocessing for the data.\\nSecond, I try to imporve the previous result, so I make some data preprocessing like removing the stop words, special charts...\\nI find out if I use pre-trained words embedding(glove), no don't need to do the step I mentioned above. I think the reason is due to the special type in twitter.\\nI also try to use LSTM-CNN model instead, but the result is worse then CNN_LSTM model.\\nBert:\\nFinally I tried Bert model, but first I add the data preprocessing I mentioned above. Regularize special type of twitter.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Data Preprocessing:\n",
    "Replace some special text by regularize words.\n",
    "CNN-LSTM model:\n",
    "First I didn't do any preprocessing for the data.\n",
    "Second, I try to imporve the previous result, so I make some data preprocessing like removing the stop words, special charts...\n",
    "I find out if I use pre-trained words embedding(glove), no don't need to do the step I mentioned above. I think the reason is due to the special type in twitter.\n",
    "I also try to use LSTM-CNN model instead, but the result is worse then CNN_LSTM model.\n",
    "Bert:\n",
    "Finally I tried Bert model, but first I add the data preprocessing I mentioned above. Regularize special type of twitter.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_dir = \"dm20-lab2-nthu/tweets_DM.json\"\n",
    "sample_dir = \"dm20-lab2-nthu/sampleSubmission.csv\"\n",
    "emotion_dir = \"dm20-lab2-nthu/emotion.csv\"\n",
    "identify_dir = \"dm20-lab2-nthu/data_identification.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9a9661149da5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweets_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"series\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1087\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m-> 1089\u001b[1;33m                 \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1090\u001b[0m             )\n\u001b[0;32m   1091\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "tweets_data = pd.read_json(tweets_dir,lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = pd.DataFrame(list(tweets_data['_source'].apply(lambda x:x['tweet']['tweet_id'])),columns=['id'])\n",
    "data_text = pd.DataFrame(list(tweets_data['_source'].apply(lambda x:x['tweet']['text'])),columns=['text']) \n",
    "data = pd.concat([data_id,data_text],axis=1)\n",
    "data = data.sort_values(by=['id'])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "#save sorted data for next time loading\n",
    "data.to_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                               text\n",
      "0  0x1c7f0f  @JZED74 While inappropriate AF, he likely wasn...\n",
      "1  0x1c7f10   o m g Shut Up And Dance though #BlackMirror <LH>\n",
      "2  0x1c7f11  On #twitch <LH> on the #Destinybeta #Destiny #...\n",
      "3  0x1c7f12  I tried to figure out why you mean so much to ...\n",
      "4  0x1c7f13  The only ‚Äúbig plan‚Äù you ever had in your life,...                id                                               text\n",
      "1867530  0x38fe19     Where is #WesHoolahan?!  #WALvIRL #COYBIG <LH>\n",
      "1867531  0x38fe1a  @mattmfm Fake news! <LH> propagated by Tumpkin...\n",
      "1867532  0x38fe1b  I told myself I'd be twitter famous. twitter m...\n",
      "1867533  0x38fe1c                    ..today was brutal  ..#Hungover\n",
      "1867534  0x38fe1d  Love it when I sun burn my forehead!! NOT!! üò´üò±...\n"
     ]
    }
   ],
   "source": [
    "print(data.head(),data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and testing data by indentification.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify = pd.read_csv(identify_dir)\n",
    "emotion = pd.read_csv(emotion_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort csv by id\n",
    "identify = identify.sort_values(by=['tweet_id'])\n",
    "identify = identify.reset_index(drop=True)\n",
    "emotion = emotion.sort_values(by=['tweet_id'])\n",
    "emotion = emotion.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['identification']=identify['identification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1c7f0f</td>\n",
       "      <td>@JZED74 While inappropriate AF, he likely wasn...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1c7f10</td>\n",
       "      <td>o m g Shut Up And Dance though #BlackMirror &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1c7f11</td>\n",
       "      <td>On #twitch &lt;LH&gt; on the #Destinybeta #Destiny #...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1c7f12</td>\n",
       "      <td>I tried to figure out why you mean so much to ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1c7f13</td>\n",
       "      <td>The only ‚Äúbig plan‚Äù you ever had in your life,...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text identification\n",
       "0  0x1c7f0f  @JZED74 While inappropriate AF, he likely wasn...           test\n",
       "1  0x1c7f10   o m g Shut Up And Dance though #BlackMirror <LH>          train\n",
       "2  0x1c7f11  On #twitch <LH> on the #Destinybeta #Destiny #...          train\n",
       "3  0x1c7f12  I tried to figure out why you mean so much to ...           test\n",
       "4  0x1c7f13  The only ‚Äúbig plan‚Äù you ever had in your life,...           test"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.loc[lambda x:x['identification']=='train']\n",
    "train_data = train_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1c7f10</td>\n",
       "      <td>o m g Shut Up And Dance though #BlackMirror &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1c7f11</td>\n",
       "      <td>On #twitch &lt;LH&gt; on the #Destinybeta #Destiny #...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1c7f14</td>\n",
       "      <td>A nice sunny wak this morning not many &lt;LH&gt; ar...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1c7f15</td>\n",
       "      <td>I'm one of those people who love candy corn......</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1c7f16</td>\n",
       "      <td>@metmuseum What are these? They look like some...</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text identification  \\\n",
       "0  0x1c7f10   o m g Shut Up And Dance though #BlackMirror <LH>          train   \n",
       "1  0x1c7f11  On #twitch <LH> on the #Destinybeta #Destiny #...          train   \n",
       "2  0x1c7f14  A nice sunny wak this morning not many <LH> ar...          train   \n",
       "3  0x1c7f15  I'm one of those people who love candy corn......          train   \n",
       "4  0x1c7f16  @metmuseum What are these? They look like some...          train   \n",
       "\n",
       "        emotion  \n",
       "0           joy  \n",
       "1  anticipation  \n",
       "2           joy  \n",
       "3           joy  \n",
       "4       disgust  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['emotion']=emotion['emotion']\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_pickle('train_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1c7f0f</td>\n",
       "      <td>@JZED74 While inappropriate AF, he likely wasn...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1c7f12</td>\n",
       "      <td>I tried to figure out why you mean so much to ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1c7f13</td>\n",
       "      <td>The only ‚Äúbig plan‚Äù you ever had in your life,...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1c7f17</td>\n",
       "      <td>Looking back on situations old &amp; new, recent o...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1c7f18</td>\n",
       "      <td>@jasoninthehouse Why do you insist on talking ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text identification\n",
       "0  0x1c7f0f  @JZED74 While inappropriate AF, he likely wasn...           test\n",
       "1  0x1c7f12  I tried to figure out why you mean so much to ...           test\n",
       "2  0x1c7f13  The only ‚Äúbig plan‚Äù you ever had in your life,...           test\n",
       "3  0x1c7f17  Looking back on situations old & new, recent o...           test\n",
       "4  0x1c7f18  @jasoninthehouse Why do you insist on talking ...           test"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = data.loc[lambda x:x['identification']=='test']\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_pickle('test_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('train_data.pkl')\n",
    "test_data = pd.read_pickle('test_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "anger            39867\n",
       "anticipation    248935\n",
       "disgust         139101\n",
       "fear             63999\n",
       "joy             516017\n",
       "sadness         193437\n",
       "surprise         48729\n",
       "trust           205478\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby(['emotion']).count()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the histogram of the data\n",
    "labels = train_data['emotion'].unique()\n",
    "post_total = len(train_data)\n",
    "df1 = train_data.groupby(['emotion']).count()['text']\n",
    "df1 = df1.apply(lambda x: round(x*100/post_total,3))\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.bar(df1.index,df1.values)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution')\n",
    "plt.grid(True)\n",
    "#plt.savefig('Emotion distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras import regularizers, initializers, optimizers, callbacks\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import MaxPooling1D,Dropout, LSTM, GRU, Bidirectional, Embedding,Conv1D\n",
    "from keras.models import Model,Sequential, load_model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('train_data.pkl')\n",
    "test_data = pd.read_pickle('test_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "''' replace some special text by regularize words \n",
    "* EX: @Yalda ===> USER\n",
    "* EX: 25 times ===> NUM times \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace some special text by regularize words\n",
    "#EX: @Yalda ===> USER , 25 times ===> NUM times ...etc\n",
    "emailsRegex=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "userMentionsRegex=re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
    "urlsRegex=re.compile('r(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+')\n",
    "numsRegex=re.compile(r\"\\b\\d+\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME: %f 21.38380241394043\n"
     ]
    }
   ],
   "source": [
    "texts = train_data['text']\n",
    "start = time.time()\n",
    "texts = texts.apply(lambda a:a.replace(\"<LH>\",\"\"))\n",
    "texts = texts.apply(lambda a:userMentionsRegex.sub(' USER',a))\n",
    "texts = texts.apply(lambda a:emailsRegex.sub(' EMAIL',a))\n",
    "texts = texts.apply(lambda a:urlsRegex.sub(' URL',a))\n",
    "texts = texts.apply(lambda a:numsRegex.sub(' NUM',a))\n",
    "print(\"TIME: %f\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(train_data['text'])\n",
    "labels = list(train_data['emotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n"
     ]
    }
   ],
   "source": [
    "label_encoder  = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "label_classes = label_encoder.classes_\n",
    "print(label_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape: (1455563, 8)\n"
     ]
    }
   ],
   "source": [
    "label = label_encoder.transform(labels)\n",
    "label = to_categorical(np.asarray(label))\n",
    "print('label shape:', label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : 36.59 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#num_words = 10000\n",
    "#tokenizer = Tokenizer(num_words=num_words, lower=True)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print(\"Time : {:.2f} sec\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 912661 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding the Large Max length\n",
    "# count_len=[]\n",
    "# for i in range(len(sequences)):\n",
    "#     count_len.append(len(sequences[i]))\n",
    "# max_len = max(count_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (1455563, 60)\n"
     ]
    }
   ],
   "source": [
    "max_len=60 #testing data max length\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "print('data shape:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test split\n",
    "* I split the training validation data with the same random seed every time for comparing which method is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I split the training validation data with the same random seed every time for finding which method is better.\n",
    "x_train,x_test,y_train,y_test = train_test_split(data, label, test_size=0.25, random_state=20191118)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained words embedding (glove)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe\n",
      "Done.\n",
      "Proceeding with Embedding Matrix...\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    " \n",
    "embeddings_index = {}\n",
    "f = open(r'glove.twitter.27B.100d.txt',encoding=\"utf-8\")\n",
    "print(\"Loading GloVe\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print(\"Done.\\nProceeding with Embedding Matrix...\")\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "''' CNN-LSTM model\n",
    "* First, I didn't do any preprocessing for the data.\n",
    "* I try to imporve it, so I make some data preprocessing like remove the stop words, special charts, @....  but the result become worse.\n",
    "* I also try to use LSTM-CNN model instead, but the result is worse then CNN_LSTM model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 60, 100)           91266200  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 60, 140)           95760     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 58, 100)           42100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 29, 100)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 29, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2900)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                145050    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 408       \n",
      "=================================================================\n",
      "Total params: 91,549,518\n",
      "Trainable params: 91,549,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_lstm1 = Bidirectional(LSTM(70,dropout=0.4,recurrent_dropout=0.4,return_sequences=True))(embedded_sequences)\n",
    "l_cov1= Conv1D(100, 3, activation='relu',kernel_regularizer=regularizers.l2(0.02))(l_lstm1)\n",
    "l_pool1 = MaxPooling1D(2)(l_cov1)\n",
    "l_drop1 = Dropout(0.4)(l_pool1)\n",
    "l_flat = Flatten()(l_drop1)\n",
    "l_dense = Dense(50, activation='relu')(l_flat)\n",
    "preds = Dense(8, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input,preds)\n",
    "adam= optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, amsgrad=False)\n",
    "#adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 50\n",
    "batch_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only save the best model.\n",
    "path = \"model/best.h5\"\n",
    "checkpoint = callbacks.ModelCheckpoint(path, monitor='val_acc',verbose=1, save_best_only=True)\n",
    "csv_logger = callbacks.CSVLogger(\"model/model_history.csv\", separator=',',append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history =model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "                         epochs=epoch, batch_size=batch_size,callbacks=[checkpoint,csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Loss/Acc \n",
    "import matplotlib.pyplot as plt \n",
    "plt.plot(train_history.history['acc'])\n",
    "plt.plot(train_history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "#plt.savefig(\"acc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/acc_4.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submition csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(test_data['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(texts)\n",
    "test = pad_sequences(test_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model/best.h5')\n",
    "predict = model.predict(test,batch_size=batch_size)\n",
    "pre = np.argmax(predict, axis=1)\n",
    "result = label_encoder.inverse_transform(pre)\n",
    "test_data['emotion'] = result\n",
    "test_data=test_data.drop(['identification','text'],axis=1)\n",
    "test_data.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not sure do I need to make the same sequence like sample submission, so I create this one.\n",
    "sample_dir = \"dm20-lab2-nthu/sampleSubmission.csv\"\n",
    "sample = pd.read_csv(sample_dir,index_col='id')\n",
    "t = pd.read_csv(\"test.csv\",index_col='id')\n",
    "sample['emotion']=t['emotion']\n",
    "sample.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from keras_bert import get_base_dict, get_model, compile_model, gen_batch_inputs\n",
    "from keras_bert import get_pretrained, PretrainedList, get_checkpoint_paths\n",
    "from keras_bert import extract_embeddings, POOL_NSP, POOL_MAX\n",
    "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Bert model:\n",
    "I do the data preprocessing I mentioned above (EX: @Yalda ===> USER , 25 times ===> NUM times ...etc). Regularize special type of twitter.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'cased_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = 'cased_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = 'cased_L-12_H-768_A-12/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = load_vocabulary(dict_path)\n",
    "tokenizer = Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sents = []\n",
    "index_sents = []\n",
    "for i, sent in tqdm(enumerate(texts)):\n",
    "    indices, segments = tokenizer.encode(sent)\n",
    "    token_sents.append(tokenizer.tokenize(sent))\n",
    "    index_sents.append(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=200\n",
    "data = pad_sequences(index_sents, maxlen=max_len,padding='post')\n",
    "x_train,x_val,y_train,y_val = train_test_split(data, label, test_size=0.25, random_state=20201204)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "m_train = np.zeros_like(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_val = np.array(x_val)\n",
    "m_val = np.zeros_like(x_val)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = load_trained_model_from_checkpoint(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    training=True,\n",
    "    trainable=True,\n",
    "    seq_len=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class=8# change\n",
    "\n",
    "inputs = bert_model.inputs[:2]\n",
    "dense = bert_model.layers[-3].output\n",
    "outputs = Dense(num_class, activation='softmax',name='predict')(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, None, 768),  22268928    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 768)    0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, None, 768)    0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, None, 768)    0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, None, 768)    0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, None, 768)    0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, None, 768)    0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, None, 768)    0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, None, 768)    0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, None, 768)    0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, None, 768)    0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, None, 768)    1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, None, 768)    0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, None, 768)    1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, None, 768)    0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, None, 768)    1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "predict (Dense)                 (None, 8)            6152        NSP-Dense[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 108,316,424\n",
      "Trainable params: 108,316,424\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs,outputs)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(1e-5),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers, callbacks\n",
    "filepath = \"model/model-{epoch:02d}-{val_acc:.2f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc',verbose=1, save_best_only=True)\n",
    "csv_logger = callbacks.CSVLogger(\"model/bert_history.csv\", separator=',',append=True)\n",
    "\n",
    "train_history =model.fit([x_train,m_train], y_train, validation_data=([x_val,m_val], y_val),\n",
    "                         epochs=epoch, batch_size=batch_size,callbacks=[checkpoint,csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle('test_data.pkl')\n",
    "test_texts = test_data['text']\n",
    "start = time.time()\n",
    "test_texts = test_texts.apply(lambda a:a.replace(\"<LH>\",\"\"))\n",
    "test_texts = test_texts.apply(lambda a:userMentionsRegex.sub(' USER',a))\n",
    "test_texts = test_texts.apply(lambda a:emailsRegex.sub(' EMAIL',a))\n",
    "test_texts = test_texts.apply(lambda a:urlsRegex.sub(' URL',a))\n",
    "test_texts = test_texts.apply(lambda a:numsRegex.sub(' NUM',a))\n",
    "print(\"TIME: %f\",time.time()-start)\n",
    "test_texts = list(test_texts)\n",
    "\n",
    "test_token_sents = []\n",
    "test_index_sents = []\n",
    "for i, sent in tqdm(enumerate(test_texts)):\n",
    "    indices, segments = tokenizer.encode(sent)\n",
    "    test_token_sents.append(tokenizer.tokenize(sent))\n",
    "    test_index_sents.append(indices)\n",
    "\n",
    "test = pad_sequences(test_index_sents, maxlen=max_len,padding='post')\n",
    "m_test = np.zeros_like(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import get_custom_objects\n",
    "model = load_model('model/model-03-0.64.h5', custom_objects=get_custom_objects())\n",
    "predict = model.predict([test,m_test],batch_size=batch_size)\n",
    "pre = np.argmax(predict, axis=1)\n",
    "result = label_encoder.inverse_transform(pre)\n",
    "\n",
    "\n",
    "test_data['emotion'] = result\n",
    "df=test_data.drop(['identification','text'],axis=1)\n",
    "df.to_csv('test.csv',index=False)\n",
    "sample_dir = \"dm20-lab2-nthu/sampleSubmission.csv\"\n",
    "sample = pd.read_csv(sample_dir,index_col='id')\n",
    "t = pd.read_csv(\"test.csv\",index_col='id')\n",
    "sample['emotion']=t['emotion']\n",
    "sample.to_csv('result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
